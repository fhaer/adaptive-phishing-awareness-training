import os
import re
import json
import asyncio
import ollama
import llm
import openai

# LLM API IDs
LLM_API_OLLAMA = 0
LLM_API_LLM_LIB = 1
LLM_API_OPENAI = 2

# LLM API to query
LLM_API = LLM_API_OLLAMA

# Default model for the selected API 
# E.g.: "qwen2.5:7b-instruct-q4_K_M" for Ollama, "gpt-4o" for OpenAI
DEFAULT_LLM_ID = "qwen2.5:7b-instruct-q4_K_M"

# OpenAI API key (required when selecting LLM_API_OPENAI)
OPENAI_API_KEY = ""

# Path to the JSON file containing prompt templates
PROMPT_TEMPLATE_FILE= "prompting/prompt_templates.json"

class LLMAPI():
    """
    A class that provides interfaces to various LLM APIs and manages conversations.
    """

    def __init__(self):
        self.conversation_by_model = {}
        self.prompt_templates = {}


    def llm_local_query_ollama(self, prompt: str, model_id: str = DEFAULT_LLM_ID, model_config: dict = {}):
        """
        Sends a query to a local Ollama model and maintains conversation history.
        
        Args:
            prompt (str): The input prompt for the model
            model_id (str): The identifier for the Ollama model to use
            model_config (dict): Configuration parameters for the model, e.g., model_config = {'temperature': 0.2}
            
        Returns:
            str: The response text from the model
        """
        
        # Initialize conversation for this model if it doesn't exist
        if model_id not in self.conversation_by_model:
            self.conversation_by_model[model_id] = []
        
        # Generate a response using Ollama
        response = ollama.chat(model=model_id, messages=self.conversation_by_model[model_id] + [{'role': 'user', 'content': prompt}], options=model_config)
        response_text = response['message']['content']
        
        # Append user and assistant messages to maintain conversation history
        self.conversation_by_model[model_id].append({'role': 'user', 'content': prompt})
        self.conversation_by_model[model_id].append({'role': 'assistant', 'content': response_text})
        
        return response_text


    def llm_local_query_lib_llm(self, prompt: str, model_id: str = DEFAULT_LLM_ID, model_config: dict = {}):
        """
        Sends a prompt to the specified LLM model using the llm library synchronously.
        
        Args:
            prompt (str): The input prompt for the model.
            model_id (str, optional): The identifier for the model to be used. If none, the api determines the default model.
            model_config (dict, optional): Configuration parameters for the model, e.g. to {'temperature': 0.2}
        
        Returns:
            str: The response generated by the model.
        """

        # Initialize conversation for this model if it doesn't exist
        if not model_id in self.conversation_by_model:
            model = llm.get_model(model_id)
            self.conversation_by_model[model_id] = model.conversation()
        conversation = self.conversation_by_model[model_id]

        # Get response from the model
        response = conversation.prompt(prompt, **model_config)
        
        # Concatenate response chunks into a single string
        response_text = ""
        for chunk in response:
            response_text += chunk

        return response_text
    

    async def llm_local_query_lib_llm_async(self, prompt: str, model_id: str = None, model_config: dict = {}):
        """
        Sends a prompt to the specified LLM model using the llm library asynchronously.
        
        Args:
            prompt (str): The input prompt for the model.
            model_id (str, optional): The identifier for the model to be used. If none, the api determines the default model.
            model_config (dict, optional): Configuration parameters for the model, e.g. to {'temperature': 0.2}
        
        Returns:
            str: The response generated by the model.
        """

        # Initialize conversation for this model if it doesn't exist
        if not model in self.conversation_by_model:
            model = llm.get_async_model(model_id)
            self.conversation_by_model[model] = model.conversation()
        conversation = self.conversation_by_model[model]

        # Get response asynchronously
        response = await conversation.prompt(prompt, **model_config)
        
        return await response.text()
    

    def reset_conversation(self, model=None):
        """
        Resets the conversation context for the given model or, if no model is specified, for all models
        
        Args:
            model (str, optional): The model identifier whose conversation should be reset
        """
        if model and model in self.conversation_by_model:
            del self.conversation_by_model[model]
        elif model is None:
            self.conversation_by_model = {}


    def llm_remote_query_openai(self, prompt: str, model_id: str = DEFAULT_LLM_ID, model_config: dict = {}, system_prompt="You are an AI assistant."):
        """
        Sends a query to OpenAI's API.
        
        Args:
            prompt (str): The input prompt for the model
            model_id (str): The identifier for the OpenAI model to use
            model_config (dict): Configuration parameters for the model
            system_prompt (str): The system prompt to set context for the model
            
        Returns:
            str: The response text from the model
        """

        # Initialize conversation for this model if it doesn't exist
        if model_id not in self.conversation_by_model:
            self.conversation_by_model[model_id] = []
        
        # Generate a response
        response = openai.ChatCompletion.create(
            model=model_id,
            messages=
                self.conversation_by_model[model_id] + 
                [{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}],
            api_key=OPENAI_API_KEY
        )
        response_text = response["choices"][0]["message"]["content"]

        # Append user and assistant messages to maintain conversation history
        if len(self.conversation_by_model[model_id]) == 0:
            self.conversation_by_model[model_id].append({'role': 'system', 'content': system_prompt})
        self.conversation_by_model[model_id].append({'role': 'user', 'content': prompt})
        self.conversation_by_model[model_id].append({'role': 'assistant', 'content': response_text})
        
        return response_text
    

    def llm_query(self, prompt: str, model_id: str = DEFAULT_LLM_ID, model_config: dict = {}):
        """
        Sends a query to the selected local or remote API.
        
        Args:
            prompt (str): The input prompt for the model
            model_id (str): The identifier for the Ollama model to use
            model_config (dict): Configuration parameters for the model, e.g., model_config = {'temperature': 0.2}
            
        Returns:
            str: The response text from the model
        """
        
        if LLM_API == LLM_API_OLLAMA:
            return self.llm_local_query_ollama(prompt, model_id, model_config)
        elif LLM_API == LLM_API_LLM_LIB:
            return self.llm_local_query_lib_llm(prompt, model_id, model_config)
        elif LLM_API == LLM_API_OPENAI:
            return self.llm_remote_query_openai(prompt, model_id, model_config)
        
        return "Error: Unknown LLM API selected"


    def instantiate_template(self, template, data):
        """
        Instantiates a prompt template with the provided data.
        
        Args:
            template (str): The template string with placeholders
            data (dict): Dictionary with values to replace placeholders
            
        Returns:
            str: The template with placeholders replaced by values
        """

        class SafeFormatDict(dict):
            def __missing__(self, key):
                # Leave undefined placeholders to be filled later
                return f'{{{key}}}'  
        
        return template.format_map(SafeFormatDict(data))


    def extract_prompt_templates(self, file_path=PROMPT_TEMPLATE_FILE):
        """
        Extracts prompt templates from the given JSON file.
        
        Args:
            file_path (str): Path to the JSON file containing templates
            
        Returns:
            dict: Dictionary of extracted templates
        """

        extracted_templates = {}
        
        if not os.path.exists(file_path):
            print(f"File '{file_path}' does not exist.")
            return extracted_templates
        
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
                
                for entry in data:
                    template_id = entry.get("template-id")
                    template = entry.get("template")
                    instruction = entry.get("instruction")
                    
                    if template_id and template and instruction:
                        extracted_templates[template_id] = self.instantiate_template(template, { "instruction": instruction })
        except (json.JSONDecodeError, FileNotFoundError, IOError) as e:
            print(f"Error processing {file_path}: {e}")

        self.prompt_templates = extracted_templates


    def get_prompt_templates(self):
        """
        Loads prompt templates from file if not already loaded, and returns them.
        
        Returns:
            dict: Dictionary of prompt templates
        """
        if not self.prompt_templates:
            self.extract_prompt_templates()
        return self.prompt_templates


    def extract_code_block(self, text):
        """
        Extracts a code block from markdown-formatted text such as from an LLM response.
        
        Args:
            text (str): Markdown text containing code blocks
            
        Returns:
            str or None: Extracted code or None if no code block found
        """
        pattern = r"```(?:\w+)?\n(.*?)```"
        match = re.search(pattern, text, re.DOTALL)
        return match.group(1).strip() if match else None


    def extract_json_code_block(self, text):
        """
        Extracts a JSON code block from markdown-formatted text and parses it.
        
        Args:
            text (str): Markdown text containing JSON code blocks
            
        Returns:
            dict: Parsed JSON object or empty dict if parsing fails
        """

        json_str = text

        code = self.extract_code_block(text)
        if code:
            json_str = code
        
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            return {}
